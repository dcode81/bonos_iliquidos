# -*- coding: utf-8 -*-
"""Imports para armado de la BD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_6cP-UJVeNwZztD_3EmFiSFH-o9jjL30
"""

import requests
import pandas as pd
import json
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import warnings
import os
import glob
import zipfile
from functools import reduce

global_token = None

def obtener_token():

    url = "https://api.invertironline.com/token"
    payload = {
        "username": "monica29",
        "password": "Fede&2410",
        "grant_type": "password"
    }
    headers = {
        "Content-Type": "application/x-www-form-urlencoded"
    }

    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        token_data = response.json()
        print("Token obtained successfully:")
        return token_data['access_token']
    else:
        print(f"Error obtaining token: {response.status_code}")
        print(response.text)

def obtener_listado_bonos(token_val):
    url = "https://api.invertironline.com/api/v2/Cotizaciones/titulosPublicos/argentina/Todos"
    headers = {
        "Authorization": f"Bearer {token_val}"
    }
    params = {
        "cotizacionInstrumentoModel.instrumento": "titulosPublicos",
        "cotizacionInstrumentoModel.pais": "argentina"
    }

    response = requests.get(url, headers=headers, params=params)
    if response.status_code != 200:
        print(f"Error obtaining data: {response.status_code}")
        print(response.text)
        return None

    # response.json() ya devuelve un diccionario, no un string
    data = response.json()

    # Extraer la lista de títulos
    titulos = data['titulos']

    # Crear una lista para almacenar solo los datos que necesitas
    rows = []

    # Campos que quieres mantener
    campos_deseados = ['simbolo', 'ultimoPrecio', 'volumen', 'descripcion']

    for titulo in titulos:
        # Crear una fila solo con los campos deseados
        row = {campo: titulo.get(campo) for campo in campos_deseados}
        rows.append(row)

    # Crear el DataFrame solo con los campos deseados
    df = pd.DataFrame(rows)

    # Mostrar información del DataFrame
    #print("Shape del DataFrame:", df.shape)
    #print("\nColumnas disponibles:")
    #print(df.columns.tolist())
    #print("\nDataFrame completo:")
    #print(df)

    # Mostrar tipos de datos
    #print("\nTipos de datos:")
    #print(df.dtypes)

    return df


def obtener_serie_historica(titulo, fecha_desde, fecha_hasta, token):
    """
    Obtiene la serie histórica de cotizaciones de un título específico.

    Parámetros:
    - titulo: string, símbolo del título (ej: 'AL30', 'GD30', etc.)
    - fecha_desde: string, fecha de inicio en formato 'YYYY-MM-DD'
    - fecha_hasta: string, fecha de fin en formato 'YYYY-MM-DD'
    - token: string, token de autorización Bearer

    Retorna:
    - DataFrame con los datos históricos o None si hay error
    """

    # Construir la URL con los parámetros
    url = f"https://api.invertironline.com/api/v2/bCBA/Titulos/{titulo}/Cotizacion/seriehistorica/{fecha_desde}/{fecha_hasta}/sinAjustar"

    # Headers con autorización Bearer
    headers = {
        "Authorization": f"Bearer {token}"
    }

    try:
        # Realizar la petición
        response = requests.get(url, headers=headers)

        # Verificar si la respuesta fue exitosa
        if response.status_code != 200:
            print(f"Error obtaining data: {response.status_code}")
            print(response.text)
            return None

        # Obtener los datos JSON
        data = response.json()

        # Verificar si hay datos en la respuesta
        if not data or len(data) == 0:
            print("No se encontraron datos para el período especificado")
            return None

        # Crear el DataFrame
        df = pd.DataFrame(data)


        # Convertir la columna fechaHora a formato YYYY-MM-DD si existe
        if 'fechaHora' in df.columns:
            df['fechaHora'] = pd.to_datetime(df['fechaHora'], format='mixed').dt.date

        # Mostrar información del DataFrame
        #print(f"Serie histórica de {titulo}")
        #print(f"Período: {fecha_desde} a {fecha_hasta}")
        #print(f"Shape del DataFrame: {df.shape}")
        #print(f"\nColumnas disponibles: {df.columns.tolist()}")
        #print(f"\nPrimeras filas:")
        #print(df.head())

        return df

    except requests.exceptions.RequestException as e:
        print(f"Error en la petición: {e}")
        return None
    except Exception as e:
        print(f"Error inesperado: {e}")
        return None



def procesar_datos_por_fecha(df: pd.DataFrame) -> pd.DataFrame:
    """
    Procesa un DataFrame para obtener el último precio y la cantidad máxima de operaciones
    por día.

    Args:
        df (pd.DataFrame): El DataFrame de entrada con las columnas
                           'fechaHora', 'ultimoPrecio' y 'cantidadOperaciones'.

    Returns:
        pd.DataFrame: Un nuevo DataFrame con las columnas 'fecha',
                      'ultimoPrecio' (del final del día) y
                      'cantidadOperaciones' (la cantidad máxima del día).
                      Devuelve None si el DataFrame de entrada está vacío o no
                      contiene las columnas requeridas.
    """
    # Manejar casos de entrada vacía o nula
    if df is None or df.empty:
        warnings.warn("El DataFrame de entrada está vacío o es None.")
        return None

    # Verificar que las columnas necesarias existen
    required_cols = ['fechaHora', 'ultimoPrecio', 'cantidadOperaciones']
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        warnings.warn(f"El DataFrame no tiene las columnas requeridas: {missing}")
        return None

    try:
        df_temp = df.copy()

        # Convertir 'fechaHora' a datetime y limpiar datos inválidos
        df_temp['fechaHora'] = pd.to_datetime(df_temp['fechaHora'], errors='coerce')
        df_temp.dropna(subset=['fechaHora'], inplace=True)

        # Crear la columna de fecha sin la hora
        df_temp['fecha'] = df_temp['fechaHora'].dt.date

        # Agrupar por fecha y obtener el índice de la última hora
        idx_ultima_hora = df_temp.groupby('fecha')['fechaHora'].idxmax()

        # Usar los índices para obtener la fila completa con el 'ultimoPrecio'
        df_final_precio = df_temp.loc[idx_ultima_hora, ['fecha', 'ultimoPrecio']]

        # Agrupar por fecha y obtener el valor máximo de 'cantidadOperaciones'
        df_max_cantidad = df_temp.groupby('fecha')['cantidadOperaciones'].max().reset_index()

        # Unir ambos DataFrames en la columna 'fecha'
        df_final = pd.merge(df_final_precio, df_max_cantidad, on='fecha')

        return df_final.reset_index(drop=True)

    except Exception as e:
        warnings.warn(f"Ocurrió un error inesperado durante el procesamiento: {e}")
        return None



def comprimir_archivos_excel_a_zip(directorio: str, nombre_zip: str):
    """
    Busca archivos .xlsx en un directorio y los comprime en un archivo .zip.

    Args:
        directorio (str): La ruta del directorio a buscar.
        nombre_zip (str): El nombre del archivo .zip de salida.
    """
    # Construye la ruta completa para el archivo zip de salida
    ruta_zip = os.path.join(directorio, nombre_zip)

    # Usa glob para encontrar todos los archivos .xlsx en el directorio
    archivos_excel = glob.glob(os.path.join(directorio, "*.xlsx"))

    if not archivos_excel:
        print(f"No se encontraron archivos .xlsx en el directorio: {directorio}")
        return

    try:
        # Crea el archivo zip en modo de escritura ('w')
        with zipfile.ZipFile(ruta_zip, 'w', zipfile.ZIP_DEFLATED) as archivo_zip:
            for archivo in archivos_excel:
                # Agrega cada archivo al .zip
                # El segundo argumento es para que el archivo se guarde en la raíz del zip
                archivo_zip.write(archivo, os.path.basename(archivo))
                print(f"Archivo agregado: {os.path.basename(archivo)}")

        print(f"\nTodos los archivos .xlsx se han comprimido correctamente en: {ruta_zip}")

    except Exception as e:
        print(f"Ocurrió un error al crear el archivo zip: {e}")


def consolidar_archivos_excel(directorio: str) -> pd.DataFrame:
    """
    Recorre un directorio, procesa los archivos .xlsx y los une en un
    único DataFrame con columnas prefijadas por el nombre del archivo.

    Args:
        directorio (str): La ruta del directorio que contiene los archivos .xlsx.

    Returns:
        pd.DataFrame: El DataFrame consolidado. Retorna None si no se encuentran
                      archivos o si ocurre un error.
    """
    # 1. Obtener la lista de archivos .xlsx
    archivos_excel = glob.glob(os.path.join(directorio, "*.xlsx"))

    if not archivos_excel:
        print(f"No se encontraron archivos .xlsx en el directorio: {directorio}")
        return None

    lista_dfs = []

    # 2. Leer y procesar cada archivo
    for archivo in archivos_excel:
        try:
            # Leer el archivo en un DataFrame temporal
            df_temp = pd.read_excel(archivo)

            # Procesar el DataFrame para agrupar por fecha
            #df_procesado = procesar_datos_por_fecha(df_temp)
            df_procesado = df_temp.copy()

            if df_procesado is not None:
                # Obtener el nombre del archivo sin la extensión para usarlo como prefijo
                nombre_archivo = os.path.splitext(os.path.basename(archivo))[0]

                # Renombrar las columnas 'ultimoPrecio' y 'cantidadOperaciones'
                df_procesado.rename(columns={
                    'ultimoPrecio': f'{nombre_archivo}_ultimoPrecio',
                    'cantidadOperaciones': f'{nombre_archivo}_cantidadOperaciones'
                }, inplace=True)

                lista_dfs.append(df_procesado)
                print(f"Archivo procesado y agregado: {archivo}")
        except Exception as e:
            print(f"Error al procesar el archivo {archivo}: {e}")
            continue

    if not lista_dfs:
        print("No se pudieron procesar DataFrames válidos.")
        return None

    # 3. Unir todos los DataFrames en uno solo
    # Usamos reduce y un merge externo para asegurarnos de que todas las fechas
    # de todos los archivos estén incluidas.
    df_consolidado = reduce(lambda left, right: pd.merge(left, right, on='fecha', how='outer'), lista_dfs)

    return df_consolidado.sort_values(by='fecha').reset_index(drop=True)

global_token = obtener_token()
lista_bonos = obtener_listado_bonos(global_token)

fecha_inicio = (datetime.now() - timedelta(days=720)).strftime('%Y-%m-%d')
fecha_fin    = datetime.now().strftime('%Y-%m-%d')

print(f"Fechas: {fecha_inicio} a {fecha_fin}")
print(f"total de bonos = {lista_bonos.size}")
# Iterar sobre las filas del DataFrame lista_bonos
for index, row in lista_bonos.iterrows():
    simbolo = row['simbolo']
    file_name = f"historico_{simbolo}.xlsx"

    try:
        df = obtener_serie_historica(simbolo, fecha_inicio, fecha_fin, global_token)
        df = procesar_datos_por_fecha(df)
        if df is not None: # Check if df is not None after processing
          df.to_excel(file_name, index=False)
          print(f"{index} - {file_name} .. procesado")
        else:
          print(f"{index} - No se pudieron obtener o procesar datos para {simbolo}")
    except Exception as e:
        print(f"{index} - Error procesando {simbolo}: {e}")
        continue # Continue to the next iteration of the loop

# Define el directorio actual donde se ejecutará el script
directorio_actual = os.getcwd()
nombre_del_zip = "historico_bonos_en_excel.zip"
comprimir_archivos_excel_a_zip(directorio_actual, nombre_del_zip)

directorio_actual = os.getcwd()
df_final = consolidar_archivos_excel(directorio_actual)

if df_final is not None:
    print("\nDataFrame Consolidado:")
    print(df_final.head())
    print("\nColumnas del DataFrame consolidado:")
    print(df_final.columns)
    df_final.to_excel("historico_bonos_en_un_solo_archivo.xlsx", index=False)

"""# Idenfificación de Bonos Iliquidos"""

#from google.colab import drive
#drive.mount('/content/drive')

#/content/drive/MyDrive/Lic Ciencia de Datos/3er Cuatrimestre/Proyecto Final/historico_bonos_en_un_solo_archivo.xlsx

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

datos = pd.read_excel('/content/drive/MyDrive/Lic Ciencia de Datos/3er Cuatrimestre/Proyecto Final/historico_bonos_en_un_solo_archivo.xlsx')
df = datos.copy()

#df.columns
for col in df.columns:
    print(col)

# Versión concisa
def mostrar_nans(df):
    total_filas = len(df)
    nan_info = df.isnull().sum().sort_values(ascending=False)

    print(f"Total de filas: {total_filas}\n")
    print(f"{'Columna':<25} {'NaN':<8} {'Total':<8} {'Porcentaje':<10}")
    print("-" * 55)

    for columna, nan_count in nan_info.items():
        porcentaje = (nan_count / total_filas * 100)
        print(f"{columna:<25} {nan_count:<8} {total_filas:<8} {porcentaje:<10.2f}%")

# Usar
mostrar_nans(df)

# Crear df_operaciones
df_operaciones = df.filter(regex='cantidadOperaciones')

# Calcular la suma de cada columna y ordenar de menor a mayor
sumas_columnas = df_operaciones.sum().sort_values(ascending=True)

# Crear el histograma
plt.figure(figsize=(12, 6))
plt.bar(range(len(sumas_columnas)), sumas_columnas.values)
plt.xlabel('Columnas')
plt.ylabel('Suma Total')
plt.title('Suma Total por Columna (Cantidad de Operaciones)')

# Configurar etiquetas del eje x
plt.xticks(range(len(sumas_columnas)), sumas_columnas.index, rotation=45, ha='right')

# Ajustar layout para que no se corten las etiquetas
plt.tight_layout()
plt.show()

# Mostrar los valores
print("Suma por columna (ordenado de menor a mayor):")
for col, suma in sumas_columnas.items():
    print(f"{col}: {suma:,.0f}")



dfEF25 = df[['fecha', 'historico_EF25D_ultimoPrecio', 'historico_EF25D_cantidadOperaciones']]
dfEF25_filtered = dfEF25[dfEF25['fecha'] >= '2024-10-01']
dfEF25_filtered

# Usando el mismo DataFrame de ejemplo
plt.figure(figsize=(10, 6))
plt.plot(dfEF25_filtered.index, dfEF25_filtered['historico_EF25D_cantidadOperaciones'], marker='o', linestyle='-', color='blue')
plt.title('Gráfico de Líneas con Discontinuidades')
plt.xlabel('Fecha')
plt.ylabel('Valor')
plt.grid(True)
plt.show()